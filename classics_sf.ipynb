{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597298193745",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the list of short stories\n",
    "\n",
    "Solving the [science fiction anthology problem](https://auxiliarymemory.com/2018/08/18/the-mathematics-of-buying-science-fiction-anthologies/) first requires importing the [list of classic short stories v2](https://csfquery.com/SearchResult?mincite=8&category=story&sortby=7&list=1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "import pandas as pd\n",
    "\n",
    "res = requests.get('https://csfquery.com/SearchResult?mincite=8&category=story&sortby=7&list=1') #Download the webpage\n",
    "res.raise_for_status() #Check that download was OK\n",
    "soup = bs4.BeautifulSoup(res.text) #Parse HTML\n",
    "classics_table = soup.find(\"table\", attrs={\"class\": \"table pt-3\"}) #Find the table with the list of stories\n",
    "classics_table_data = classics_table.tbody.find_all(\"a\", attrs={\"class\": \"a-csf\"}) #Find all links in the table\n",
    "\n",
    "df = pd.DataFrame(columns=['story_author', 'story_title', 'story_link']) #Create dataframe for storing the list of stories\n",
    "for i in classics_table_data: #Loop over the links found previously \n",
    "    if \"title.cgi\" in i['href']:\n",
    "        story_link = i['href']\n",
    "        story_title = i.text\n",
    "    elif \"ea.cgi\" in i['href']:\n",
    "        story_author = i.text\n",
    "        df.loc[len(df)]=([story_author,story_title,story_link]) #Append dataframe with author, title, story link\n",
    "\n",
    "df = df.groupby(['story_title','story_link'])['story_author'].apply(', '.join).reset_index() # Merge entries for stories with more than one author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a list of anthologies\n",
    "\n",
    "Having a database (`df`) of short stories, we now need to obtain the list of anthologies from ISFDB.org containing at least one story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame(columns=['story_author', 'story_title', 'story_link','publication_title','publication_link']) #Create empty dataframe which will store a mapping between stories and anthologies\n",
    "for index, story_row in df.iterrows(): #Loop over all stories\n",
    "    story_author = story_row['story_author']\n",
    "    story_title = story_row['story_title']\n",
    "    story_link = story_row['story_link']\n",
    "    story_link += '+1' #Change the ISFDB link to \"Do not display translations\"\n",
    "    res = requests.get(story_link)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    publications_table = soup.find(\"table\", attrs={\"class\": \"publications\"}) #Find the table listing all publications of a story\n",
    "    publications_df = pd.read_html(str(publications_table), header=0)[0] #Read the HTML table into a dataframe (there could be more tables, but we're interested in the first, indexed 0)\n",
    "    publications_table_links = publications_table.find_all(\"a\") #Extract all HTML links from the table\n",
    "    publications_links = [] \n",
    "    for l in publications_table_links:\n",
    "        if \"pl.cgi\" in l['href']:\n",
    "            publications_links.append(l['href'])\n",
    "    publications_df['publications_links']=publications_links #Add column with links to anthologies to dataframe\n",
    "    publications_df = publications_df[publications_df.Format.str.contains('digital audio download')] #Only anthologies, collections, and omnibuses are of interest, drop others\n",
    "    publications_df = publications_df.drop_duplicates(subset='Title', keep=\"first\") #Removing duplicates by exactle title match (this could be improved)\n",
    "    for index2, publication_row in publications_df.iterrows(): #Iterate over all publications\n",
    "        publication_title = publication_row['Title']\n",
    "        publication_link = publication_row['publications_links']\n",
    "        # print(\"Adding to main_df: \", story_title, \" in \", publication_title)\n",
    "        main_df.loc[len(main_df)]=([story_author,story_title,story_link,publication_title,publication_link]) #Update main database, each entry (row) is a story-to-anthology mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may want to save this mapping into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_df.to_csv('main_digital_audio_downloads.csv')"
   ]
  }
 ]
}